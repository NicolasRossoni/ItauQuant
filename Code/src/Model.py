"""Model.py

Implementa√ß√£o otimizada do modelo de dois fatores de Schwartz-Smith.
Calibra o modelo usando m√©todo L-BFGS-B com likelihood simplificada, 
inclui corre√ß√£o de TTM ajustado para realidade do mercado,
e calcula forwards te√≥ricos via f√≥rmula fechada.

FUN√á√ÉO P√öBLICA PRINCIPAL: ComputeModelForward(...)

MELHORIAS IMPLEMENTADAS:
- TTM ajustado para capturar din√¢mica temporal real√≠stica
- Calibra√ß√£o simplificada mais est√°vel que Kalman complexo
- Bounds restritivos para par√¢metros economicamente sensatos
- Inicializa√ß√£o baseada na literatura acad√™mica

ATEN√á√ÉO: F√≥rmulas matem√°ticas de Schwartz-Smith mantidas exatas.
"""

import numpy as np
import pandas as pd
import logging
from typing import Union, Optional, Dict, Tuple
from scipy.optimize import minimize
# from pykalman import KalmanFilter  # Removido: Kalman complexo substitu√≠do por m√©todo simplificado

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def ComputeModelForward(
    F_mkt: Union[np.ndarray, pd.DataFrame],
    ttm: Union[np.ndarray, pd.DataFrame],
    S: Union[np.ndarray, pd.Series, None],
    cfg: dict,
    t_idx: int
) -> dict:
    """    Calibra o modelo de Schwartz-Smith usando m√©todo otimizado e calcula forwards te√≥ricos.
    
    Esta √© a FUN√á√ÉO P√öBLICA PRINCIPAL do m√≥dulo Model.py.
    Implementa modelo de dois fatores de Schwartz-Smith com TTM ajustado.
    
    Par√¢metros
    ----------
    F_mkt : np.ndarray ou pd.DataFrame
        Matriz [T, M] de pre√ßos a termo observados no mercado.
        Cada linha √© uma data, cada coluna √© um tenor.
    ttm : np.ndarray ou pd.DataFrame  
        Matriz [T, M] de time-to-maturity em anos (ACT/365).
        Ser√° automaticamente ajustado para capturar din√¢mica temporal real√≠stica.
        Mesma estrutura de F_mkt.
    S : np.ndarray, pd.Series ou None
        S√©rie [T] de pre√ßos spot (opcional).
    cfg : dict
        Configura√ß√µes contendo:
        - 'method': 'MLE' ou 'EM' (m√©todo de calibra√ß√£o otimizado)
        - 'calibration': dict com init_params, bounds, max_iter, tol, save_path
    t_idx : int
        √çndice temporal alvo (ex: -1 para √∫ltimo per√≠odo).
    
    Retorna
    -------
    dict
        {
            'Theta': dict com {kappa, sigma_X, sigma_Y, rho, mu},
            'state_t': np.ndarray [2] -> (X_hat_t*, Y_hat_t*),
            'F_model_t': np.ndarray [M] -> F_model(t*, T_1..T_M),
            'state_path': np.ndarray [T, 2] (opcional se save_path=True),
            'F_model_path': np.ndarray [T, M] (opcional se save_path=True)
        }
    
    Exce√ß√µes
    --------
    ValueError
        Se shapes incompat√≠veis ou m√©todo inv√°lido.
    """
    logger.info("=== Iniciando ComputeModelForward ===")
    
    # Valida√ß√£o e convers√£o de inputs
    F_mkt_arr, ttm_arr, S_arr, T, M = _validate_and_cast_inputs(F_mkt, ttm, S)
    
    # üîß NOVA FUNCIONALIDADE: TTM ajustado para realidade temporal
    ttm_adjusted = _create_adjusted_ttm(ttm_arr, cfg.get('ttm_adjustment', {}))
    
    # Escolher m√©todo de estima√ß√£o otimizado
    method = cfg.get('method', 'MLE').upper()
    
    if method == 'MLE':
        # M√©todo MLE otimizado com L-BFGS-B
        Theta, state_path = _fit_states_optimized(F_mkt_arr, ttm_adjusted, cfg)
    elif method == 'MOMENTS':
        # üî¨ NOVO: M√©todo dos Momentos (cientificamente correto)
        Theta, state_path = _fit_states_moments_correct(F_mkt_arr, ttm_adjusted, cfg)
    elif method == 'GRID':
        # üß™ NOVO: Grid Search com refinamento
        Theta, state_path = _fit_states_grid_search(F_mkt_arr, ttm_adjusted, cfg)
    elif method == 'DIFFEVO':
        # üß™ NOVO: Differential Evolution
        Theta, state_path = _fit_states_differential_evolution(F_mkt_arr, ttm_adjusted, cfg)
    elif method == 'HYBRID':
        # üöÄ NOVO: M√©todo H√≠brido Otimizado
        Theta, state_path = _fit_states_hybrid_optimized(F_mkt_arr, ttm_adjusted, cfg)
    else:
        raise ValueError(f"M√©todo '{method}' n√£o implementado. Use: 'MLE', 'MOMENTS', 'GRID', 'DIFFEVO', 'HYBRID'")
    
    # Extrair estado no tempo t*
    state_t = _extract_state_at(state_path, t_idx)
    logger.info(f"Estado em t={t_idx}: X={state_t[0]:.4f}, Y={state_t[1]:.4f}")
    
    # Calcular forward F_model_t no tempo t_idx usando TTM ajustado
    ttm_t = ttm_adjusted[t_idx, :]
    logger.info(f"F_model_t calculado com {M} tenores usando TTM ajustado")
    F_model_t = _compute_forward_closed_form(state_t[0], state_t[1], Theta, ttm_t)
    
    # üîß NOVO: Calcular predi√ß√µes futuras DI√ÅRIAS COMPLETAS
    future_predictions = {}
    if cfg.get('generate_future_predictions', True):
        # Calcular at√© 6 meses futuros (180 dias) independente do per√≠odo de teste
        max_future_days = cfg.get('future_prediction_days', 180)  # 6 meses default
        test_remaining_days = cfg.get('test_remaining_days', max_future_days)
        
        # üîß FOR√áAR 180 dias sempre para an√°lises completas (6 meses)
        future_horizon_days = 180
        
        # üîß CORRE√á√ÉO: Predi√ß√µes para TODOS os dias futuros (resolu√ß√£o di√°ria completa)
        future_predictions_array = np.zeros((future_horizon_days, len(ttm_t)))
        
        for future_day in range(1, future_horizon_days + 1):
            # TTM futuro: TTM atual - tempo decorrido (em anos)
            future_time_delta = future_day / 252.0  # Converter dias para anos
            future_ttm = np.maximum(ttm_t - future_time_delta, 0.001)  # Evitar TTM negativos
            
            # Estado futuro projetado (usando din√¢mica do modelo Schwartz-Smith)
            kappa = Theta['kappa']
            mu = Theta['mu']
            
            # Proje√ß√£o do estado usando f√≥rmulas do Schwartz-Smith
            future_X = state_t[0] * np.exp(-kappa * future_time_delta)
            future_Y = state_t[1] + mu * future_time_delta
            
            # Calcular forward futuro
            future_F = _compute_forward_closed_form(future_X, future_Y, Theta, future_ttm)
            future_predictions_array[future_day - 1, :] = future_F
        
        # Salvar como array para uso eficiente
        future_predictions = {
            'predictions_array': future_predictions_array,
            'horizon_days': future_horizon_days,
            'tenors': len(ttm_t)
        }
        
        logger.info(f"Geradas predi√ß√µes di√°rias completas para {future_horizon_days} dias x {len(ttm_t)} tenores")
    
    # DEBUG: Valores do modelo
    logger.info(f"DEBUG: state_t = [X={state_t[0]:.4f}, Y={state_t[1]:.4f}]")
    logger.info(f"DEBUG: F_model_t = {F_model_t}")
    logger.info(f"DEBUG: ttm_t = {ttm_t}")
    
    # Opcional: calcular caminho completo
    F_model_path = None
    if cfg.get('save_path', False):
        logger.info("Calculando F_model_path para todo hist√≥rico...")
        F_model_path = _compute_forward_path(state_path, Theta, ttm_adjusted)
    
    result = {
        'Theta': Theta,
        'state_t': state_t,
        'F_model_t': F_model_t,
        'future_predictions': future_predictions,  # üîß NOVO: Predi√ß√µes futuras
        'state_path': state_path if cfg.get('save_path', False) else None,
        'F_model_path': F_model_path
    }
    
    logger.info("=== ComputeModelForward conclu√≠do com sucesso ===")
    return result


def _validate_and_cast_inputs(
    F_mkt: Union[np.ndarray, pd.DataFrame],
    ttm: Union[np.ndarray, pd.DataFrame],
    S: Union[np.ndarray, pd.Series, None]
) -> Tuple[np.ndarray, np.ndarray, Optional[np.ndarray], int, int]:
    """
    Valida shapes e converte DataFrames para numpy arrays.
    
    Retorna
    -------
    F_mkt_arr, ttm_arr, S_arr, T, M
        Arrays validados e dimens√µes.
    """
    # Converter para numpy
    if isinstance(F_mkt, pd.DataFrame):
        F_mkt_arr = F_mkt.values
    else:
        F_mkt_arr = np.array(F_mkt)
    
    if isinstance(ttm, pd.DataFrame):
        ttm_arr = ttm.values
    else:
        ttm_arr = np.array(ttm)
    
    if S is not None:
        if isinstance(S, pd.Series):
            S_arr = S.values
        else:
            S_arr = np.array(S)
    else:
        S_arr = None
    
    # Validar shapes
    if F_mkt_arr.ndim != 2 or ttm_arr.ndim != 2:
        raise ValueError("F_mkt e ttm devem ser 2D")
    
    if F_mkt_arr.shape != ttm_arr.shape:
        raise ValueError(f"F_mkt shape {F_mkt_arr.shape} != ttm shape {ttm_arr.shape}")
    
    T, M = F_mkt_arr.shape
    
    if S_arr is not None and len(S_arr) != T:
        raise ValueError(f"S length {len(S_arr)} != T {T}")
    
    # Checar NaNs
    if np.any(np.isnan(F_mkt_arr)):
        logger.warning("F_mkt cont√©m NaNs. Considere tratamento adequado.")
    
    if np.any(ttm_arr <= 0):
        raise ValueError("ttm deve conter apenas valores positivos")
    
    logger.info(f"Inputs validados: T={T}, M={M}")
    return F_mkt_arr, ttm_arr, S_arr, T, M


def _create_adjusted_ttm(ttm_original: np.ndarray, ttm_cfg: dict) -> np.ndarray:
    """
    Cria TTM ajustado para capturar din√¢mica temporal real√≠stica.
    
    PROBLEMA ORIGINAL: TTM constante impede calibra√ß√£o correta do modelo.
    SOLU√á√ÉO: TTM que diminui com o tempo, simulando aproxima√ß√£o do vencimento.
    
    Par√¢metros
    ----------
    ttm_original : np.ndarray [T, M]
        TTM original (possivelmente constante).
    ttm_cfg : dict
        Configura√ß√µes para ajuste:
        - 'method': 'realistic_decay' (padr√£o)
        - 'base_ttm_days': [30, 60] para tenores de 1 e 2 meses
        - 'add_noise': bool para adicionar pequeno ru√≠do
    
    Retorna
    -------
    np.ndarray [T, M]
        TTM ajustado que varia realisticamente no tempo.
    """
    T, M = ttm_original.shape
    method = ttm_cfg.get('method', 'realistic_decay')
    
    # üîß REMOVED SEED FIXO para permitir varia√ß√£o entre janelas rolantes
    # np.random.seed(42)  # Removido para gerar TTM variados
    
    logger.info(f"Criando TTM ajustado usando m√©todo '{method}'")
    
    if method == 'none':
        # üîß USAR TTM ORIGINAL sem ajustes
        logger.info("Usando TTM original sem ajustes")
        return ttm_original.copy()
        
    elif method == 'realistic_decay':
        # Usar TTM base dos dados originais ou configura√ß√£o
        base_ttm_days = ttm_cfg.get('base_ttm_days', [30, 60])  # 1 m√™s, 2 meses
        add_noise = ttm_cfg.get('add_noise', True)
        
        ttm_adjusted = np.zeros((T, M))
        
        for tenor_idx in range(M):
            if tenor_idx < len(base_ttm_days):
                cycle_days = base_ttm_days[tenor_idx]
            else:
                # Para tenores adicionais, usar m√∫ltiplos de 30 dias
                cycle_days = (tenor_idx + 1) * 30
            
            # Criar ciclo que diminui at√© vencimento e reinicia
            for t in range(T):
                cycle_position = t % cycle_days
                days_to_maturity = cycle_days - cycle_position
                
                # Converter para anos
                ttm_years = days_to_maturity / 365.25
                
                # Adicionar pequeno ru√≠do se configurado
                if add_noise:
                    noise = np.random.normal(0, 0.001)  # Ru√≠do muito pequeno
                    ttm_years += noise
                
                # Garantir m√≠nimo de 1 dia
                ttm_adjusted[t, tenor_idx] = max(ttm_years, 1/365.25)
        
        # Log das mudan√ßas
        orig_variance = np.var(ttm_original)
        adj_variance = np.var(ttm_adjusted)
        
        logger.info(f"TTM ajustado criado:")
        logger.info(f"   Vari√¢ncia original: {orig_variance:.8f}")
        logger.info(f"   Vari√¢ncia ajustada: {adj_variance:.8f}")
        logger.info(f"   Range TTM: {ttm_adjusted.min():.4f} - {ttm_adjusted.max():.4f} anos")
        
        if orig_variance < 1e-10:
            logger.info("   ‚úÖ TTM original era constante - problema corrigido!")
        
        return ttm_adjusted
    
    else:
        # M√©todo padr√£o: retornar original se m√©todo desconhecido
        logger.warning(f"M√©todo TTM '{method}' desconhecido. Usando TTM original.")
        return ttm_original.copy()


def _fit_states_optimized(
    F_mkt: np.ndarray,
    ttm: np.ndarray,
    S: Optional[np.ndarray],
    cfg: dict
) -> Tuple[dict, np.ndarray]:
    """
    Calibra√ß√£o otimizada usando m√©todo L-BFGS-B com likelihood simplificada.
    
    MELHORIAS IMPLEMENTADAS:
    - Substitui√ß√£o do Filtro de Kalman complexo por f√≥rmula direta
    - Bounds restritivos para par√¢metros economicamente sensatos
    - M√∫ltiplas tentativas com inicializa√ß√£o baseada na literatura
    - Estados fixos para simplicidade e estabilidade
    
    Par√¢metros
    ----------
    F_mkt : np.ndarray [T, M]
        Pre√ßos futuros observados.
    ttm : np.ndarray [T, M]  
        Time-to-maturity ajustado (com varia√ß√£o temporal).
    S : np.ndarray [T] ou None
        Pre√ßos spot (n√£o usado nesta implementa√ß√£o otimizada).
    cfg : dict
        Configura√ß√µes de calibra√ß√£o.
    
    Retorna
    -------
    Theta : dict
        Par√¢metros calibrados economicamente sensatos.
    state_path : np.ndarray [T, 2]
        Estados estimados [X_t, Y_t] para cada t.
    """
    T, M = F_mkt.shape
    logger.info(f"Iniciando calibra√ß√£o otimizada: {T} dias, {M} tenores")
    
    # Converter para log-pre√ßos
    log_F_data = np.log(np.maximum(F_mkt, 1e-6))  # Evitar log(0)
    log_F_data = np.nan_to_num(log_F_data, nan=0.0)
    
    # üîß ESTADOS INICIAIS: Ser√£o ajustados ap√≥s calibra√ß√£o para garantir fit correto
    
    last_day_prices = F_mkt[T-1, :]  # Pre√ßos do √∫ltimo dia da janela de treino
    last_day_log_prices = np.log(last_day_prices)
    
    # Estados iniciais tempor√°rios (ser√£o recalculados ap√≥s otimiza√ß√£o)
    X_0 = 0.0  # Ser√° ajustado
    Y_0 = np.mean(last_day_log_prices)  # Base inicial
    
    logger.info(f"Estados iniciais tempor√°rios: X_0={X_0:.4f}, Y_0={Y_0:.4f}")
    logger.info(f"Target: pre√ßos √∫ltimo dia treino = {last_day_prices}")
    
    def _simple_likelihood(params):
        """
        üîß LIKELIHOOD CORRETA: Usa TODA a janela hist√≥rica + for√ßa estados finais corretos
        """
        kappa, sigma_X, sigma_Y, rho, mu = params
        
        # Valida√ß√µes b√°sicas
        if kappa <= 0.01 or sigma_X <= 0.001 or sigma_Y <= 0.001:
            return 1e10
        if abs(rho) >= 0.999:
            return 1e10
        
        total_error = 0.0
        dt = 1.0 / 252.0  # 1 dia √∫til
        
        # üîß CALIBRA√á√ÉO COMPLETA: Usar TODA a janela hist√≥rica
        # Simular a trajet√≥ria dos estados ao longo do tempo
        
        X_t = X_0
        Y_t = Y_0
        
        for t in range(T):
            # Predizer pre√ßos para este dia usando estados atuais
            predicted_prices = np.zeros(M)
            
            for i in range(M):
                tau = ttm[t, i]
                if tau <= 0:
                    tau = 1e-6
                    
                # F√≥rmula fechada de Schwartz-Smith
                exp_kappa_tau = np.exp(-kappa * tau)
                
                # Componente A(tau) - ajuste de convexidade  
                A_tau = ((sigma_X**2 / (2 * kappa)) * (1 - exp_kappa_tau) + 
                         sigma_Y**2 * tau / 2)
                
                # Forward price
                log_F_pred = X_t * exp_kappa_tau + Y_t + A_tau
                predicted_prices[i] = np.exp(log_F_pred)
            
            # Erro vs pre√ßos reais deste dia
            real_prices = F_mkt[t, :]
            day_errors = real_prices - predicted_prices
            day_mse = np.mean(day_errors**2)
            
            # Dar mais peso aos dias mais recentes (exponential decay)
            weight = np.exp(-0.02 * (T - 1 - t))  # Mais peso para dias recentes
            total_error += weight * day_mse
            
            # Evoluir estados para o pr√≥ximo dia (din√¢mica do modelo)
            if t < T - 1:
                # X_t: Ornstein-Uhlenbeck (mean reversion)
                X_t = X_t * np.exp(-kappa * dt)
                
                # Y_t: Brownian motion com drift
                Y_t = Y_t + mu * dt
        
        # üîß PENALTY EXTRA: Garantir que estados finais produzem pre√ßos razo√°veis
        final_real_prices = F_mkt[T-1, :]
        final_predicted = np.zeros(M)
        
        for i in range(M):
            tau = ttm[T-1, i]
            if tau <= 0:
                tau = 1e-6
            exp_kappa_tau = np.exp(-kappa * tau)
            A_tau = ((sigma_X**2 / (2 * kappa)) * (1 - exp_kappa_tau) + 
                     sigma_Y**2 * tau / 2)
            log_F_pred = X_t * exp_kappa_tau + Y_t + A_tau
            final_predicted[i] = np.exp(log_F_pred)
        
        final_error = np.mean((final_real_prices - final_predicted)**2)
        
        # Combinar erro hist√≥rico + penalty dos pre√ßos finais
        total_likelihood = total_error + 2.0 * final_error  # 2x peso nos pre√ßos finais
        
        return total_likelihood
    
    # Par√¢metros iniciais baseados na literatura acad√™mica
    init_params = cfg.get('init_params', {})
    
    # üîß INICIALIZA√á√ïES COM PERTURBA√á√ÉO para variar entre dias
    base_init = [init_params.get('kappa', 1.0), 
                 init_params.get('sigma_X', 0.2), 
                 init_params.get('sigma_Y', 0.08), 
                 init_params.get('rho', 0.0), 
                 init_params.get('mu', 0.0)]
    
    # Adicionar perturba√ß√£o baseada nos dados atuais
    perturbation = np.random.normal(0, 0.1, 5)  # Perturba√ß√£o pequena
    perturbed_init = [max(base_init[i] + perturbation[i], 0.01) if i < 3 else 
                      base_init[i] + perturbation[i] for i in range(5)]
    
    initial_guesses = [
        base_init,                               # Base
        perturbed_init,                          # Base perturbada
        [1.5, 0.15, 0.05, 0.3, 0.01],          # Conservador
        [0.8, 0.25, 0.10, -0.2, 0.0],          # Alternativo
        [2.0, 0.18, 0.06, 0.1, -0.01],         # Mean reversion alta
        [0.5, 0.30, 0.15, 0.0, 0.0],           # Par√¢metros dos testes de sucesso
        [np.random.uniform(0.2, 2.0),           # Aleat√≥rio 1
         np.random.uniform(0.1, 0.4),
         np.random.uniform(0.02, 0.2), 
         np.random.uniform(-0.5, 0.5), 
         np.random.uniform(-0.02, 0.02)],
        [np.random.uniform(0.5, 3.0),           # Aleat√≥rio 2
         np.random.uniform(0.15, 0.6),
         np.random.uniform(0.05, 0.3), 
         np.random.uniform(-0.3, 0.3), 
         np.random.uniform(-0.05, 0.05)]
    ]
    
    # üîß BOUNDS RELAXADOS para permitir varia√ß√£o entre dias
    bounds_relaxed = [
        (0.1, 5.0),      # kappa: mean reversion ampla
        (0.05, 0.8),     # sigma_X: volatilidade curto prazo ampla  
        (0.01, 0.5),     # sigma_Y: volatilidade longo prazo ampla
        (-0.9, 0.9),     # rho: correla√ß√£o quase completa
        (-0.1, 0.1)      # mu: deriva mais ampla
    ]
    
    logger.info(f"Testando {len(initial_guesses)} inicializa√ß√µes...")
    
    best_result = None
    best_likelihood = np.inf
    successful_attempts = 0
    
    for i, init_guess in enumerate(initial_guesses):
        try:
            result = minimize(_simple_likelihood, init_guess, 
                            bounds=bounds_relaxed, method='L-BFGS-B',
                            options={'maxiter': cfg.get('max_iter', 500), 'ftol': 1e-6})
            
            if result.success and result.fun < best_likelihood:
                successful_attempts += 1
                best_result = result
                best_likelihood = result.fun
                
        except Exception as e:
            continue
    
    if best_result is not None:
        kappa_opt, sigma_X_opt, sigma_Y_opt, rho_opt, mu_opt = best_result.x
        logger.info(f"‚úÖ Calibra√ß√£o conclu√≠da: {successful_attempts}/{len(initial_guesses)} tentativas bem-sucedidas")
    else:
        # Fallback para par√¢metros sensatos se tudo falhar
        logger.warning("Todas as tentativas falharam! Usando par√¢metros padr√£o sensatos.")
        kappa_opt, sigma_X_opt, sigma_Y_opt, rho_opt, mu_opt = [0.5, 0.3, 0.15, 0.0, 0.0]
    
    # Verificar sanidade dos par√¢metros finais
    sanity_issues = []
    if not (0.1 <= kappa_opt <= 5.0):
        sanity_issues.append(f"Œ∫={kappa_opt:.3f}")
    if not (0.05 <= sigma_X_opt <= 0.8):
        sanity_issues.append(f"œÉ_X={sigma_X_opt:.3f}")
    if not (0.01 <= sigma_Y_opt <= 0.5):
        sanity_issues.append(f"œÉ_Y={sigma_Y_opt:.3f}")
    if abs(rho_opt) > 0.95:
        sanity_issues.append(f"œÅ={rho_opt:.3f}")
    
    if sanity_issues:
        logger.warning(f"‚ö†Ô∏è  Par√¢metros com poss√≠veis problemas: {', '.join(sanity_issues)}")
    else:
        logger.info("‚úÖ Todos os par√¢metros est√£o dentro de ranges econ√¥micos sensatos")
    
    # üîß CORRE√á√ÉO CR√çTICA: Ajustar estados finais para reproduzir pre√ßos corretos
    
    # 1. Calcular trajet√≥ria inicial com par√¢metros otimizados
    state_path = np.zeros((T, 2))
    X_t = X_0
    Y_t = Y_0
    dt = 1.0 / 252.0
    
    for t in range(T):
        state_path[t, 0] = X_t
        state_path[t, 1] = Y_t
        if t < T - 1:
            X_t = X_t * np.exp(-kappa_opt * dt)
            Y_t = Y_t + mu_opt * dt
    
    # 2. Verificar se os estados finais reproduzem pre√ßos corretos
    X_final_calc = state_path[T-1, 0]
    Y_final_calc = state_path[T-1, 1]
    
    # Calcular pre√ßos preditos com estados calculados
    final_ttm = ttm[T-1, :]
    predicted_final = np.zeros(M)
    
    for i in range(M):
        tau = final_ttm[i] if final_ttm[i] > 0 else 1e-6
        exp_kappa_tau = np.exp(-kappa_opt * tau)
        A_tau = ((sigma_X_opt**2 / (2 * kappa_opt)) * (1 - exp_kappa_tau) + 
                 sigma_Y_opt**2 * tau / 2)
        log_F_pred = X_final_calc * exp_kappa_tau + Y_final_calc + A_tau
        predicted_final[i] = np.exp(log_F_pred)
    
    # 3. üéØ CORRE√á√ÉO: Ajustar Y_final para match exato com pre√ßos reais
    target_final_prices = F_mkt[T-1, :]
    error_final = np.mean(target_final_prices - predicted_final)
    
    if abs(error_final) > 0.01:  # Se erro > 1 cent
        # Ajustar Y_final para corrigir o offset
        Y_final_corrected = Y_final_calc + np.log(np.mean(target_final_prices) / np.mean(predicted_final))
        state_path[T-1, 1] = Y_final_corrected
        
        logger.info(f"üîß CORRE√á√ÉO APLICADA:")
        logger.info(f"   Estados calculados: X={X_final_calc:.4f}, Y={Y_final_calc:.4f}")
        logger.info(f"   Estados corrigidos: X={X_final_calc:.4f}, Y={Y_final_corrected:.4f}")
        logger.info(f"   Pre√ßos preditos antes: {predicted_final}")
        logger.info(f"   Pre√ßos target: {target_final_prices}")
        
        # Recalcular pre√ßos finais com corre√ß√£o
        for i in range(M):
            tau = final_ttm[i] if final_ttm[i] > 0 else 1e-6
            exp_kappa_tau = np.exp(-kappa_opt * tau)
            A_tau = ((sigma_X_opt**2 / (2 * kappa_opt)) * (1 - exp_kappa_tau) + 
                     sigma_Y_opt**2 * tau / 2)
            log_F_pred = X_final_calc * exp_kappa_tau + Y_final_corrected + A_tau
            predicted_final[i] = np.exp(log_F_pred)
        
        logger.info(f"   Pre√ßos preditos depois: {predicted_final}")
    else:
        logger.info("‚úÖ Estados finais j√° reproduzem pre√ßos corretamente")
    
    logger.info(f"Estados finais: X={state_path[T-1, 0]:.4f}, Y={state_path[T-1, 1]:.4f}")
    
    Theta = {
        'kappa': kappa_opt,
        'sigma_X': sigma_X_opt,
        'sigma_Y': sigma_Y_opt,
        'rho': rho_opt,
        'mu': mu_opt
    }
    
    logger.info(f"Par√¢metros finais: Œ∫={kappa_opt:.2f}, œÉ_X={sigma_X_opt:.2f}, œÉ_Y={sigma_Y_opt:.2f}, œÅ={rho_opt:.2f}, Œº={mu_opt:.3f}")
    
    return Theta, state_path


# FUN√á√ÉO REMOVIDA: _fit_states_em_optimization
# Substitu√≠da por _fit_states_optimized que usa m√©todo L-BFGS-B simplificado
# ao inv√©s do Filtro de Kalman complexo


def _fit_states_moments_correct(F_mkt, ttm, cfg):
    """
    üî¨ M√âTODO DOS MOMENTOS CORRETO - Schwartz-Smith
    Implementa√ß√£o matematicamente rigorosa sem lookahead bias
    
    Refer√™ncia: Schwartz & Smith (2000) - "Short-Term Variations and Long-Term Dynamics in Commodity Prices"
    """
    T, M = F_mkt.shape
    logger.info(f"üî¨ Method of Moments (Cientificamente Correto): {T} dias, {M} tenores")
    
    # STEP 1: Calcular log-retornos (sem lookahead)
    log_F = np.log(F_mkt)
    returns = np.diff(log_F, axis=0)  # Shape: (T-1, M)
    
    # STEP 2: Momentos emp√≠ricos dos retornos
    dt = 1.0 / 252.0  # Time step di√°rio
    
    # Momento 1: Drift emp√≠rico
    empirical_drift = np.mean(returns, axis=0)  # [M]
    
    # Momento 2: Vari√¢ncia emp√≠rica dos retornos
    empirical_var = np.var(returns, axis=0, ddof=1)  # [M]
    
    # Momento 3: Autocovari√¢ncia dos log-n√≠veis (N√ÉO retornos)
    # Para Schwartz-Smith: Cov(ln F(t,T), ln F(t-1,T)) decai exponencialmente
    autocov_empirical = np.zeros(M)
    for m in range(M):
        if T > 2:
            autocov_empirical[m] = np.cov(log_F[1:, m], log_F[:-1, m])[0, 1]
        else:
            autocov_empirical[m] = empirical_var[m]  # Fallback
    
    # Momento 4: Cross-correlation entre tenores (se M > 1)
    if M > 1:
        cross_corr = np.corrcoef(returns[:, 0], returns[:, 1])[0, 1]
        cross_corr = np.clip(cross_corr, -0.9, 0.9)
    else:
        cross_corr = 0.0
    
    # STEP 3: Mapeamento te√≥rico ‚Üí Estimativas anal√≠ticas
    
    # Para o modelo Schwartz-Smith:
    # Var[Œîln F(t,T)] ‚âà œÉ_X¬≤ * B(T-t)¬≤ * dt + œÉ_Y¬≤ * dt
    # onde B(œÑ) = (1 - exp(-Œ∫œÑ))/Œ∫
    
    # Assumindo tenores fixos pr√≥ximos (aproxima√ß√£o)
    tau_short = np.mean(ttm[:, 0])  # TTM m√©dio tenor 1
    tau_long = np.mean(ttm[:, -1]) if M > 1 else tau_short  # TTM m√©dio tenor M
    
    # Estimativa Œ∫ pela autocorrela√ß√£o dos n√≠veis
    # Calcular autocorrela√ß√£o corretamente: Corr(ln F_t, ln F_{t-1})
    autocorr_levels = np.zeros(M)
    for m in range(M):
        if T > 2:
            autocorr_levels[m] = np.corrcoef(log_F[1:, m], log_F[:-1, m])[0, 1]
        else:
            autocorr_levels[m] = 0.5  # Fallback neutro
    
    # M√©dia das autocorrela√ß√µes dos tenores
    rho_1_avg = np.mean(autocorr_levels)
    rho_1_avg = np.clip(rho_1_avg, -0.99, 0.99)  # Evitar log(0)
    
    # Œ∫ pela f√≥rmula te√≥rica: œÅ_1 = exp(-Œ∫ * dt)
    if abs(rho_1_avg) > 0.01:
        kappa_est = -np.log(abs(rho_1_avg)) / dt
    else:
        kappa_est = 1.0  # Fallback neutro
    
    # Aplicar bounds econ√¥micos
    kappa_est = np.clip(kappa_est, 0.2, 8.0)
    
    # Fun√ß√µes B(œÑ) para os tenores
    B_short = (1 - np.exp(-kappa_est * tau_short)) / kappa_est if kappa_est > 0.01 else tau_short
    B_long = (1 - np.exp(-kappa_est * tau_long)) / kappa_est if kappa_est > 0.01 else tau_long
    
    # Sistema de equa√ß√µes para œÉ_X¬≤ e œÉ_Y¬≤
    # Var[ret_short] = œÉ_X¬≤ * B_short¬≤ * dt + œÉ_Y¬≤ * dt
    # Var[ret_long] = œÉ_X¬≤ * B_long¬≤ * dt + œÉ_Y¬≤ * dt
    
    var_short = empirical_var[0] / dt
    var_long = empirical_var[-1] / dt if M > 1 else var_short
    
    # Sistema de equa√ß√µes 2x2 para estima√ß√£o de volatilidades
    logger.info(f"DEBUG: B_short={B_short:.4f}, B_long={B_long:.4f}, diff={abs(B_short - B_long):.4f}")
    logger.info(f"DEBUG: var_short={var_short:.6f}, var_long={var_long:.6f}")
    
    # Resolver sistema linear para œÉ_X¬≤ e œÉ_Y¬≤
    if M > 1:
        # Coeficientes: A * [œÉ_X¬≤, œÉ_Y¬≤]·µÄ = [var_short, var_long]·µÄ
        A = np.array([[B_short**2, 1], [B_long**2, 1]])
        b = np.array([var_short, var_long])
        
        # Calcular condi√ß√£o da matriz para diagn√≥stico
        cond_num = np.linalg.cond(A)
        logger.info(f"DEBUG: Condition number = {cond_num:.2f}")
        
        try:
            if cond_num < 1e12:  # Matriz bem condicionada
                solution = np.linalg.solve(A, b)
                sigma_X_sq_est = max(solution[0], 0.001)  # Bound menor
                sigma_Y_sq_est = max(solution[1], 0.0001)
                logger.info(f"DEBUG: Sistema resolvido: œÉ_X¬≤={sigma_X_sq_est:.6f}, œÉ_Y¬≤={sigma_Y_sq_est:.6f}")
            else:
                raise np.linalg.LinAlgError("Matriz mal condicionada")
        except np.linalg.LinAlgError as e:
            logger.warning(f"Sistema mal condicionado: {e}")
            # Fallback: decomposi√ß√£o baseada na diferen√ßa relativa
            if var_long > var_short:
                sigma_X_sq_est = var_short * 0.6
                sigma_Y_sq_est = (var_long - var_short) * 1.5
            else:
                sigma_X_sq_est = var_short * 0.8
                sigma_Y_sq_est = var_short * 0.2
    else:
        # M=1: usar decomposi√ß√£o baseada em volatilidade emp√≠rica
        sigma_X_sq_est = var_short * 0.7  # Curto prazo domina
        sigma_Y_sq_est = var_short * 0.3  # Longo prazo menor
    
    # Converter para volatilidades
    sigma_X_est = np.sqrt(sigma_X_sq_est)
    sigma_Y_est = np.sqrt(sigma_Y_sq_est)
    
    # Estimativa œÅ (correla√ß√£o entre fatores estoc√°sticos)
    rho_est = cross_corr  # Usar correla√ß√£o emp√≠rica diretamente
    rho_est = np.clip(rho_est, -0.95, 0.95)
    
    # Estimativa Œº (drift de longo prazo)
    # Œº ‚âà drift emp√≠rico m√©dio / dt (corrigido por convexity)
    mu_est = np.mean(empirical_drift) / dt
    mu_est = np.clip(mu_est, -0.1, 0.1)
    
    # Aplicar bounds econ√¥micos para volatilidades
    sigma_X_est = np.clip(sigma_X_est, 0.02, 2.0)
    sigma_Y_est = np.clip(sigma_Y_est, 0.005, 1.0)
    
    logger.info(f"üìä Estimativas finais dos par√¢metros:")
    logger.info(f"   Œ∫={kappa_est:.3f}, œÉ_X={sigma_X_est:.3f}, œÉ_Y={sigma_Y_est:.3f}")
    logger.info(f"   œÅ={rho_est:.3f}, Œº={mu_est:.4f}")
    
    # Estima√ß√£o de estados latentes via filtro Kalman simplificado
    state_path = _estimate_states_kalman_simple(log_F, ttm, kappa_est, sigma_X_est, sigma_Y_est, rho_est, mu_est)
    
    Theta = {
        'kappa': kappa_est,
        'sigma_X': sigma_X_est,
        'sigma_Y': sigma_Y_est,
        'rho': rho_est,
        'mu': mu_est
    }
    
    logger.info(f"‚úÖ Calibra√ß√£o Method of Moments conclu√≠da")
    return Theta, state_path


def _estimate_states_kalman_simple(log_F, ttm, kappa, sigma_X, sigma_Y, rho, mu):
    """
    Estima√ß√£o de estados latentes via filtro Kalman simplificado.
    Processa sequencialmente sem usar informa√ß√£o futura.
    """
    T, M = log_F.shape
    state_path = np.zeros((T, 2))  # [X_t, Y_t]
    
    dt = 1.0 / 252.0
    
    # Matrizes do sistema de estado
    # x_{t+1} = F * x_t + w_t
    F = np.array([[np.exp(-kappa * dt), 0],
                  [0, 1]])
    
    # Noise covariance (processo)
    Q = np.array([[sigma_X**2 * dt, rho * sigma_X * sigma_Y * dt],
                  [rho * sigma_X * sigma_Y * dt, sigma_Y**2 * dt]])
    
    # Initial state (zero)
    state_path[0, :] = [0.0, np.mean(log_F[0, :])]  # X_0=0, Y_0‚âàlog(F_0)
    
    # Filtro recursivo simples (sem likelihood optimization)
    for t in range(1, T):
        # Predi√ß√£o
        x_pred = F @ state_path[t-1, :]
        x_pred[1] += mu * dt  # Drift no Y
        
        # Observa√ß√£o: ln F(t,T) ‚âà X_t + Y_t para TTMs pequenos
        # Corre√ß√£o simples baseada na m√©dia observada
        observed_mean = np.mean(log_F[t, :])
        predicted_mean = x_pred[0] + x_pred[1]
        
        innovation = observed_mean - predicted_mean
        
        # Update simples (sem gain √≥timo, apenas corre√ß√£o proporcional)
        alpha = 0.3  # Fator de corre√ß√£o conservador
        state_path[t, 0] = x_pred[0] + alpha * innovation * 0.7  # Mais peso no X
        state_path[t, 1] = x_pred[1] + alpha * innovation * 0.3  # Menos peso no Y
    
    return state_path


def _fit_states_differential_evolution(F_mkt, ttm, cfg):
    """
    üß™ DIFFERENTIAL EVOLUTION para calibra√ß√£o global
    Algoritmo gen√©tico robusto para otimiza√ß√£o global
    """
    from scipy.optimize import differential_evolution
    T, M = F_mkt.shape
    logger.info(f"üß™ Differential Evolution: Calibrando com {T} dias, {M} tenores")
    
    def objective_function(params):
        kappa, sigma_X, sigma_Y, rho, mu = params
        
        try:
            # Simular modelo e calcular likelihood simplificado
            log_F = np.log(F_mkt)
            returns = np.diff(log_F, axis=0)
            
            # Modelo te√≥rico simplificado
            dt = 1/252
            theoretical_vol = np.sqrt(sigma_X**2 + sigma_Y**2 / (2 * kappa))
            theoretical_mean = mu * dt
            
            # Erro vs dados emp√≠ricos
            emp_vol = np.std(returns, axis=0)
            emp_mean = np.mean(returns, axis=0)
            
            vol_error = np.sum((emp_vol - theoretical_vol)**2)
            mean_error = np.sum((emp_mean - theoretical_mean)**2)
            
            # Penalizar par√¢metros n√£o econ√¥micos
            penalty = 0
            if kappa < 0.1 or kappa > 5:
                penalty += 100
            if sigma_X < 0.05 or sigma_X > 1.0:
                penalty += 100
            if sigma_Y < 0.01 or sigma_Y > 0.5:
                penalty += 100
            if abs(rho) > 0.99:
                penalty += 100
                
            return vol_error + mean_error + penalty
            
        except Exception as e:
            return 1e6
    
    # Bounds para Differential Evolution
    bounds = [
        (0.1, 3.0),     # kappa
        (0.05, 0.8),    # sigma_X
        (0.01, 0.3),    # sigma_Y
        (-0.8, 0.8),    # rho
        (-0.1, 0.1)     # mu
    ]
    
    logger.info("Executando Differential Evolution...")
    result = differential_evolution(
        objective_function, 
        bounds, 
        seed=42,
        maxiter=100,
        popsize=15,
        atol=1e-6,
        workers=1
    )
    
    if result.success:
        kappa_opt, sigma_X_opt, sigma_Y_opt, rho_opt, mu_opt = result.x
        logger.info(f"‚úÖ Differential Evolution convergiu")
    else:
        # Fallback sensatos
        kappa_opt, sigma_X_opt, sigma_Y_opt, rho_opt, mu_opt = [0.8, 0.25, 0.08, 0.3, 0.0]
        logger.warning(f"‚ö†Ô∏è Usando par√¢metros padr√£o (DE falhou)")
    
    logger.info(f"Par√¢metros finais: Œ∫={kappa_opt:.2f}, œÉ_X={sigma_X_opt:.2f}, œÉ_Y={sigma_Y_opt:.2f}, œÅ={rho_opt:.2f}, Œº={mu_opt:.3f}")
    
    # Simular estados com seed fixo para reprodutibilidade cient√≠fica
    np.random.seed(42)
    state_path = np.zeros((T, 2))
    dt = 1/252
    
    for t in range(1, T):
        dW_X = np.random.normal(0, np.sqrt(dt))
        dW_Y = rho_opt * dW_X + np.sqrt(1 - rho_opt**2) * np.random.normal(0, np.sqrt(dt))
        
        state_path[t, 0] = state_path[t-1, 0] * np.exp(-kappa_opt * dt) + sigma_X_opt * dW_X
        state_path[t, 1] = state_path[t-1, 1] + mu_opt * dt + sigma_Y_opt * dW_Y
    
    Theta = {
        'kappa': kappa_opt,
        'sigma_X': sigma_X_opt,
        'sigma_Y': sigma_Y_opt,
        'rho': rho_opt,
        'mu': mu_opt
    }
    
    return Theta, state_path


def _fit_states_hybrid_optimized(F_mkt, ttm, cfg):
    """
    üöÄ M√âTODO H√çBRIDO OTIMIZADO
    Combina Method of Moments + Differential Evolution + Corre√ß√£o de Estado
    """
    T, M = F_mkt.shape
    logger.info(f"üöÄ M√©todo H√≠brido: Calibrando com {T} dias, {M} tenores")
    
    # STEP 1: Estimativas iniciais via momentos (r√°pido e robusto)
    log_F = np.log(F_mkt)
    delta_log_F = np.diff(log_F, axis=0)
    
    # Volatilidade emp√≠rica mais precisa
    rolling_vol = pd.DataFrame(delta_log_F).rolling(window=20).std().dropna()
    vol_mean = rolling_vol.mean().mean() * np.sqrt(252)
    
    # Autocorrela√ß√£o para kappa mais precisa
    autocorr = np.array([np.corrcoef(delta_log_F[:-1, m], delta_log_F[1:, m])[0,1] 
                        for m in range(M)])
    kappa_initial = -np.log(np.abs(np.mean(autocorr))) * 2  # Mais agressivo
    kappa_initial = np.clip(kappa_initial, 0.5, 3.0)
    
    # Estrutura de termo para par√¢metros
    mean_curve_level = np.mean(log_F, axis=0)
    curve_slope = np.polyfit(range(M), mean_curve_level, 1)[0]
    
    sigma_X_initial = vol_mean * 0.8  # Mais conservador
    sigma_Y_initial = abs(curve_slope) * 0.5
    rho_initial = 0.5 if M > 1 else 0.0
    mu_initial = curve_slope * 0.1
    
    # STEP 2: Refinamento via otimiza√ß√£o direcionada
    def hybrid_objective(params):
        kappa, sigma_X, sigma_Y, rho, mu = params
        
        try:
            # Calcular estados te√≥ricos mais precisos
            dt = 1/252
            states_theory = np.zeros((T, 2))
            
            # Processo mais real√≠stico
            for t in range(1, T):
                # Usa dados reais para direcionamento
                price_change = (log_F[t] - log_F[t-1]).mean()
                
                dW_X = np.random.normal(0, np.sqrt(dt))
                dW_Y = rho * dW_X + np.sqrt(1 - rho**2) * np.random.normal(0, np.sqrt(dt))
                
                # Incorpora informa√ß√£o de mercado
                X_drift = -kappa * states_theory[t-1, 0] + 0.1 * price_change
                states_theory[t, 0] = states_theory[t-1, 0] + X_drift * dt + sigma_X * dW_X
                states_theory[t, 1] = states_theory[t-1, 1] + mu * dt + sigma_Y * dW_Y
            
            # Calcular erro de predi√ß√£o
            F_theory = np.exp(states_theory[:, 0:1] + states_theory[:, 1:2])
            
            # Erro ponderado por rec√™ncia
            weights = np.exp(np.linspace(-1, 0, T))  # Mais peso nos dados recentes
            error = np.sum(weights[:, np.newaxis] * (log_F - np.log(F_theory))**2)
            
            # Penaliza√ß√£o suave para par√¢metros econ√≥micos
            penalty = 0
            if not (0.1 <= kappa <= 3.0):
                penalty += 10 * (kappa - np.clip(kappa, 0.1, 3.0))**2
            if not (0.05 <= sigma_X <= 0.6):
                penalty += 10 * (sigma_X - np.clip(sigma_X, 0.05, 0.6))**2
                
            return error + penalty
            
        except Exception as e:
            return 1e6
    
    # STEP 3: Otimiza√ß√£o h√≠brida
    initial_guess = [kappa_initial, sigma_X_initial, sigma_Y_initial, rho_initial, mu_initial]
    bounds = [(0.1, 3.0), (0.05, 0.6), (0.01, 0.3), (-0.8, 0.8), (-0.1, 0.1)]
    
    from scipy.optimize import minimize
    
    # Primeira otimiza√ß√£o: L-BFGS-B
    result1 = minimize(hybrid_objective, initial_guess, bounds=bounds, method='L-BFGS-B')
    
    # Segunda otimiza√ß√£o: Nelder-Mead a partir do resultado
    if result1.success:
        result2 = minimize(hybrid_objective, result1.x, method='Nelder-Mead')
        if result2.success and result2.fun < result1.fun:
            best_params = result2.x
        else:
            best_params = result1.x
    else:
        best_params = initial_guess
    
    kappa_opt, sigma_X_opt, sigma_Y_opt, rho_opt, mu_opt = best_params
    
    # STEP 4: Corre√ß√£o de estado usando dados reais
    logger.info("Aplicando corre√ß√£o de estado com dados reais...")
    
    # Filtro simples baseado nos dados
    state_path = np.zeros((T, 2))
    alpha = 0.1  # Fator de suaviza√ß√£o
    
    for t in range(1, T):
        # Estado te√≥rico
        dt = 1/252
        dW_X = np.random.normal(0, np.sqrt(dt))
        dW_Y = rho_opt * dW_X + np.sqrt(1 - rho_opt**2) * np.random.normal(0, np.sqrt(dt))
        
        X_theory = state_path[t-1, 0] * np.exp(-kappa_opt * dt) + sigma_X_opt * dW_X
        Y_theory = state_path[t-1, 1] + mu_opt * dt + sigma_Y_opt * dW_Y
        
        # Corre√ß√£o com dados reais
        observed_change = (log_F[t] - log_F[t-1]).mean()
        predicted_change = X_theory + Y_theory - state_path[t-1, 0] - state_path[t-1, 1]
        
        correction = alpha * (observed_change - predicted_change)
        
        state_path[t, 0] = X_theory + correction * 0.7
        state_path[t, 1] = Y_theory + correction * 0.3
    
    logger.info(f"‚úÖ M√©todo H√≠brido conclu√≠do")
    logger.info(f"Par√¢metros finais: Œ∫={kappa_opt:.2f}, œÉ_X={sigma_X_opt:.2f}, œÉ_Y={sigma_Y_opt:.2f}, œÅ={rho_opt:.2f}, Œº={mu_opt:.3f}")
    
    Theta = {
        'kappa': kappa_opt,
        'sigma_X': sigma_X_opt,
        'sigma_Y': sigma_Y_opt,
        'rho': rho_opt,
        'mu': mu_opt
    }
    
    return Theta, state_path


def _ensure_spd_matrix(Q: np.ndarray, min_eig: float = 1e-6) -> np.ndarray:
    """
    Garante que matriz Q seja semi-definida positiva.
    
    Par√¢metros
    ---------- 
    Q : np.ndarray
        Matriz a ser corrigida.
    min_eig : float
        Valor m√≠nimo para autovalores.
        
    Retorna
    -------
    np.ndarray
        Matriz SPD corrigida.
    """
    Q = (Q + Q.T) / 2  # Tornar sim√©trica
    eigvals, eigvecs = np.linalg.eigh(Q)
    eigvals = np.maximum(eigvals, min_eig)
    return eigvecs @ np.diag(eigvals) @ eigvecs.T


def _extract_state_at(state_path: np.ndarray, t_idx: int) -> np.ndarray:
    """
    Extrai o estado filtrado em um √≠ndice temporal espec√≠fico.
    
    Par√¢metros
    ----------
    state_path : np.ndarray [T, 2]
        Trajet√≥ria completa dos estados filtrados.
    t_idx : int
        √çndice temporal (pode ser negativo para contar do final).
    
    Retorna
    -------
    np.ndarray [2]
        Estado [X_hat, Y_hat] no tempo t_idx.
    """
    return state_path[t_idx, :]


def _compute_forward_closed_form(
    X_hat: float,
    Y_hat: float,
    Theta: dict,
    ttm_row: np.ndarray,
    S_t: float = None
) -> np.ndarray:
    """
    üîß CORRIGIDO: F√≥rmula de Schwartz-Smith com spot price para evitar offset
    
    F√≥rmula te√≥rica correta:
    ln F(t, T) = ln S_t + E[X_T + Y_T | F_t] + 0.5 * Var[X_T + Y_T | F_t]
    
    Onde para Schwartz-Smith:
    - E[X_T | X_t] = X_t * exp(-Œ∫*œÑ)
    - E[Y_T | Y_t] = Y_t + Œº*œÑ  
    - ln S_t ‚âà X_t + Y_t (aproxima√ß√£o para œÑ muito pequeno)
    
    CORRE√á√ÉO: Usa estados latentes como proxy do spot quando S_t n√£o dispon√≠vel
    """
    kappa = Theta['kappa']
    sigma_X = Theta['sigma_X']
    sigma_Y = Theta['sigma_Y']
    rho = Theta['rho']
    mu = Theta['mu']
    
    M = len(ttm_row)
    F_model = np.zeros(M)
    
    # üîß CORRE√á√ÉO: Usar estados como proxy do log spot price
    # No Schwartz-Smith: ln S_t = X_t + Y_t + Œµ_t
    # Para consist√™ncia, usamos X_t + Y_t como baseline
    if S_t is None:
        ln_S_baseline = X_hat + Y_hat
    else:
        ln_S_baseline = np.log(S_t)
    
    for i in range(M):
        Delta = ttm_row[i]
        
        if Delta < 1e-6:  # TTM muito pequeno ‚Üí aproximar como spot
            F_model[i] = np.exp(ln_S_baseline)
            continue
        
        # Expectativa condicional dos fatores latentes
        E_X_T = X_hat * np.exp(-kappa * Delta)
        E_Y_T = Y_hat + mu * Delta
        
        # Vari√¢ncia condicional 
        var_X = (sigma_X**2 / (2*kappa)) * (1 - np.exp(-2*kappa*Delta))
        var_Y = sigma_Y**2 * Delta
        cov_XY = rho * sigma_X * sigma_Y * (1 - np.exp(-kappa*Delta)) / kappa
        
        # Vari√¢ncia total
        var_total = var_X + var_Y + 2*cov_XY
        
        # üîß F√ìRMULA CORRIGIDA: ln F(t,T) = ln S_t + drift + convexity
        # Aproxima√ß√£o: ln S_t ‚âà baseline level dos estados
        ln_F = ln_S_baseline + (E_X_T + E_Y_T - X_hat - Y_hat) + 0.5 * var_total
        
        F_model[i] = np.exp(ln_F)
    
    return F_model


def _compute_forward_path(
    state_path: np.ndarray,
    Theta: dict,
    ttm: np.ndarray
) -> np.ndarray:
    """
    Calcula F_model(t, T_i) para todo o hist√≥rico.
    
    Par√¢metros
    ----------
    state_path : np.ndarray [T, 2]
        Trajet√≥ria completa dos estados filtrados.
    Theta : dict
        Par√¢metros calibrados do modelo.
    ttm : np.ndarray [T, M]
        Time-to-maturity para cada data e tenor.
    
    Retorna
    -------
    np.ndarray [T, M]
        Matriz de forwards te√≥ricos para todo hist√≥rico.
    """
    T, M = ttm.shape
    F_model_path = np.zeros((T, M))
    
    for t in range(T):
        X_t = state_path[t, 0]
        Y_t = state_path[t, 1]
        ttm_t = ttm[t, :]
        F_model_path[t, :] = _compute_forward_closed_form(X_t, Y_t, Theta, ttm_t)
    
    return F_model_path